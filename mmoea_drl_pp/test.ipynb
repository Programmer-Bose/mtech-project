{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ccb07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab1c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 20\n",
    "embedded_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca82715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "coords = torch.rand(batch_size,seq_len,embedded_dim)*100\n",
    "print(coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c5d306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = nn.Linear(embedded_dim,embedded_dim)\n",
    "K = nn.Linear(embedded_dim,embedded_dim)\n",
    "V = nn.Linear(embedded_dim,embedded_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70a280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q(coords)\n",
    "k = K(coords)\n",
    "v = V(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9740c234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 20, 128]), torch.Size([1, 20, 128]), torch.Size([1, 20, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b03d652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff9b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_scores = torch.matmul(q,k.transpose(-2,-1)) / (embedded_dim**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822d8546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edb06f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = F.softmax(pre_scores,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14132d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df428074",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_val = torch.matmul(scores,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33942aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42e99f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "seq_len = 20\n",
    "embed_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, embed_dim=128, seq_len=20):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(seq_len, embed_dim)\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        print(positions)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.embedding(positions)  # (batch, seq_len, embed_dim)\n",
    "        x_proj = self.fc(x)                  # (batch, seq_len, embed_dim)\n",
    "        return x_proj + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20, 128])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(2,embed_dim,seq_len)\n",
    "enc_out = enc(torch.rand(batch_size,seq_len,2))\n",
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3106244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, output_dim, seq_len):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_dim + embed_dim, seq_len)\n",
    "        self.attn_combine = nn.Linear(hidden_dim + embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_input, hidden):\n",
    "        # encoder_outputs: (batch, seq_len, embed_dim)\n",
    "        # decoder_input: (batch, 1, embed_dim)\n",
    "        # hidden: (1, batch, hidden_dim)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "\n",
    "        # Repeat hidden state across seq_len for attention\n",
    "        hidden_repeat = hidden[-1].unsqueeze(1).repeat(1, self.seq_len, 1)  # (batch, seq_len, hidden_dim)\n",
    "        attn_input = torch.cat((encoder_outputs, hidden_repeat), dim=2)      # (batch, seq_len, embed_dim+hidden_dim)\n",
    "        attn_weights = F.softmax(self.attn(attn_input), dim=1)               # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.transpose(1,2), encoder_outputs)    # (batch, seq_len, embed_dim)\n",
    "        context = context[:, 0:1, :]  # Use only the first context vector for current step\n",
    "\n",
    "        # Combine context with decoder input\n",
    "        rnn_input = torch.cat((decoder_input, context), dim=2)               # (batch, 1, embed_dim*2)\n",
    "        rnn_input = self.attn_combine(rnn_input)                             # (batch, 1, embed_dim)\n",
    "        rnn_input = F.relu(rnn_input)\n",
    "\n",
    "        output, hidden = self.gru(rnn_input, hidden)                         # output: (batch, 1, hidden_dim)\n",
    "        output = self.out(output.squeeze(1))                                 # (batch, output_dim)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "decoder = DecoderWithAttention(embed_dim=embedded_dim, hidden_dim=128, output_dim=embedded_dim, seq_len=seq_len)\n",
    "decoder_input = torch.zeros(batch_size, 1, embedded_dim)  # initial input (e.g., <SOS>)\n",
    "hidden = torch.zeros(1, batch_size, 128)\n",
    "output, hidden, attn_weights = decoder(enc_out, decoder_input, hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1570ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "task_seq = [0, 3, 1, 5, 7, 2]\n",
    "task_seq = [tid for tid in task_seq if tid not in [0, 1, 2]]\n",
    "print(task_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88efe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT_DEPOTS = {\n",
    "    0: (0.1, 0.1),   # Robot 0's base\n",
    "    1: (0.9, 0.1),   # Robot 1's base\n",
    "    2: (0.5, 0.9),   # Robot 2's base\n",
    "}\n",
    "TASK_COORDINATES = {\n",
    "    0: (0.2, 0.2),   # Task 0\n",
    "    1: (0.8, 0.2),   # Task 1\n",
    "    2: (0.5, 0.8),   # Task 2   \n",
    "    3: (0.3, 0.7),   # Task 3\n",
    "    4: (0.6, 0.5),   # Task 4\n",
    "    5: (0.4, 0.4),   # Task 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d051064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.1), (0.2, 0.2), (0.3, 0.7), (0.8, 0.2), (0.4, 0.4), (0.6, 0.5), (0.5, 0.8)]\n"
     ]
    }
   ],
   "source": [
    "task_seq = [0, 3, 1, 5, 4, 2]\n",
    "depot_coord = ROBOT_DEPOTS[0]\n",
    "# task_seq = [tid for tid in task_seq if tid not in [0, 1, 2]]\n",
    "coords = [depot_coord] + [TASK_COORDINATES[tid] for tid in task_seq]\n",
    "print(coords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_mlag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

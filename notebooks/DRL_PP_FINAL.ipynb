{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab55d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e640a",
   "metadata": {},
   "source": [
    "#### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Encoder -----\n",
    "class TaskEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.ff_layer = nn.Linear(2, embedding_dim)\n",
    "        self.gru_encoder = nn.GRU(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, input_coords):\n",
    "        embedded = self.ff_layer(input_coords)\n",
    "        encoder_outputs, hidden = self.gru_encoder(embedded)\n",
    "        return embedded, encoder_outputs, hidden\n",
    "\n",
    "# ----- Attention -----\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask):\n",
    "        decoder_hidden = decoder_hidden.transpose(0, 1)  # (1, B, H) -> (B, 1, H)\n",
    "        score = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden)))  # (B, S, 1)\n",
    "        score = score.squeeze(-1)\n",
    "        score[mask == 0] = -1e9  # Mask visited\n",
    "        attn_weights = F.softmax(score, dim=-1)\n",
    "        return attn_weights\n",
    "\n",
    "# ----- Decoder -----\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, decoder_input, hidden, encoder_outputs, mask):\n",
    "        output, hidden = self.gru(decoder_input, hidden)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        return attn_weights, hidden\n",
    "\n",
    "# ----- Critic -----\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        pooled = encoder_outputs.mean(dim=1)\n",
    "        return self.fc(pooled).squeeze(-1)\n",
    "\n",
    "# ----- Training -----\n",
    "\n",
    "def train_drl_ac(embedding_dim, hidden_dim, seq_len, batch_size, lr, epochs, n_batches):\n",
    "    torch.manual_seed(42)\n",
    "    encoder = TaskEncoder(embedding_dim)\n",
    "    decoder = Decoder(embedding_dim, hidden_dim)\n",
    "    critic = Critic(hidden_dim)\n",
    "\n",
    "    actor_optim = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        input_coords = torch.rand(batch_size, seq_len, 2)\n",
    "        _, encoder_outputs, encoder_hidden = encoder(input_coords)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_reward = 0\n",
    "            mask = torch.ones(batch_size, seq_len)\n",
    "            decoder_input = torch.randn(batch_size, 1, embedding_dim)\n",
    "            hidden = encoder_hidden\n",
    "            log_probs = []\n",
    "            tour = []\n",
    "\n",
    "            for _ in range(seq_len):\n",
    "                attn_weights, hidden = decoder(decoder_input, hidden, encoder_outputs, mask.clone())\n",
    "                dist = torch.distributions.Categorical(attn_weights)\n",
    "                selected = dist.sample()\n",
    "                # selected = torch.argmax(attn_weights, dim=1)\n",
    "                log_prob = dist.log_prob(selected)\n",
    "                log_probs.append(log_prob.squeeze())\n",
    "                idx = selected.item()\n",
    "                tour.append(idx)\n",
    "                mask = mask.clone()\n",
    "                mask.scatter_(1, torch.tensor([[idx]]), 0)  # safe scatter op\n",
    "                decoder_input = encoder_outputs[0, idx].detach().clone().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "            coords = input_coords[0][tour]\n",
    "            path = torch.cat([coords, coords[0].unsqueeze(0)], dim=0)\n",
    "            reward = -torch.norm(path[1:] - path[:-1], dim=1).sum()\n",
    "            total_reward += reward.item()\n",
    "\n",
    "            value = critic(encoder_outputs)\n",
    "            advantage = reward.detach() - value\n",
    "            actor_loss = -torch.stack(log_probs).sum() * advantage.detach()\n",
    "            critic_loss = (value - reward.detach()).pow(2).mean()\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optim.step()\n",
    "            \n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            \n",
    "        # print(f\"Epoch {epoch+1}/{epochs} - Actor Loss: {actor_loss.item():.4f}, Critic Loss: {critic_loss.item():.4f}\")\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save({\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optim.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optim.state_dict()\n",
    "    }, 'tsp_actor_critic_02.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drl_ac(embedding_dim = 256, hidden_dim = 256, seq_len = 50, batch_size = 1, lr = 5e-4, epochs = 20, n_batches =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce21f82",
   "metadata": {},
   "source": [
    "#### New Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- Actor (Unified Class) -----\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Encoder part\n",
    "        self.ff_layer = nn.Linear(2, embedding_dim)\n",
    "        self.gru_encoder = nn.GRU(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
    "\n",
    "        # Decoder part (which includes Attention)\n",
    "        self.gru_decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = self._Attention(hidden_dim) # Using inner Attention class\n",
    "\n",
    "    # Inner Attention class (private to Actor, or could be a separate helper)\n",
    "    class _Attention(nn.Module):\n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        def forward(self, decoder_hidden, encoder_outputs, mask):\n",
    "            # decoder_hidden: (1, B, H)\n",
    "            # encoder_outputs: (B, S, H)\n",
    "            \n",
    "            # (1, B, H) -> (B, 1, H) for broadcasting with encoder_outputs\n",
    "            decoder_hidden_expanded = decoder_hidden.transpose(0, 1) \n",
    "            \n",
    "            # Calculate score: (B, S, H) + (B, 1, H) -> (B, S, H) -> (B, S, 1)\n",
    "            score = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_expanded)))\n",
    "            score = score.squeeze(-1) # (B, S)\n",
    "\n",
    "            # Mask visited (B, S)\n",
    "            score[mask == 0] = -1e9 \n",
    "            \n",
    "            attn_weights = F.softmax(score, dim=-1) # (B, S)\n",
    "            return attn_weights\n",
    "\n",
    "    def forward(self, input_coords, seq_len):\n",
    "        # input_coords: (B, S, 2)\n",
    "\n",
    "        # Encoder Pass\n",
    "        # embedded: (B, S, E)\n",
    "        # encoder_outputs: (B, S, H)\n",
    "        # encoder_hidden: (1, B, H)\n",
    "        embedded = self.ff_layer(input_coords)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(embedded)\n",
    "\n",
    "        batch_size = input_coords.size(0)\n",
    "        \n",
    "        # Initialize for decoding\n",
    "        # mask: (B, S) - all ones indicating all cities are unvisited\n",
    "        mask = torch.ones(batch_size, seq_len, device=input_coords.device)\n",
    "        \n",
    "        # Initial decoder input: (B, 1, E) - Using zeros as a start token embedding\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding_dim, device=input_coords.device)\n",
    "        \n",
    "        hidden = encoder_hidden # Initial decoder hidden state from encoder\n",
    "        \n",
    "        log_probs_batch = [] \n",
    "        tours_batch = [] \n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            output, hidden = self.gru_decoder(decoder_input, hidden)\n",
    "            attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "            \n",
    "            dist = torch.distributions.Categorical(attn_weights)\n",
    "            selected = dist.sample() # (B,)\n",
    "            \n",
    "            log_prob = dist.log_prob(selected) # (B,)\n",
    "            log_probs_batch.append(log_prob)\n",
    "            \n",
    "            tours_batch.append(selected)\n",
    "            \n",
    "            # Update mask: Set selected nodes to 0 for each item in the batch\n",
    "            # selected.unsqueeze(-1) changes (B,) to (B, 1) for scatter_\n",
    "            mask.scatter_(1, selected.unsqueeze(-1), 0)\n",
    "            \n",
    "            # --- CORRECT FIX for torch.gather index shape ---\n",
    "            # selected: (B,)\n",
    "            # unsqueeze(1) -> (B, 1)\n",
    "            # unsqueeze(2) -> (B, 1, 1)\n",
    "            # expand(-1, 1, self.embedding_dim) -> (B, 1, embedding_dim)\n",
    "            # This 'gather_indices' tensor now has the same number of dimensions as encoder_outputs\n",
    "            # and is correctly shaped to select an embedding vector for each batch item.\n",
    "            gather_indices = selected.unsqueeze(1).unsqueeze(2).expand(-1, 1, self.embedding_dim)\n",
    "            decoder_input = torch.gather(encoder_outputs, 1, gather_indices).detach()\n",
    "            # decoder_input will be (B, 1, E) as required for the next GRU step\n",
    "            # --- END CORRECT FIX ---\n",
    "\n",
    "        tours_batch_tensor = torch.stack(tours_batch, dim=1) # (B, seq_len)\n",
    "        log_probs_batch_tensor = torch.stack(log_probs_batch, dim=0) # (seq_len, B)\n",
    "\n",
    "        return log_probs_batch_tensor, tours_batch_tensor, encoder_outputs \n",
    "\n",
    "\n",
    "# ----- Critic -----\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        pooled = encoder_outputs.mean(dim=1) # (B, H)\n",
    "        return self.fc(pooled).squeeze(-1) # (B,)\n",
    "\n",
    "# ----- Training -----\n",
    "\n",
    "def train_drl_ac(embedding_dim, hidden_dim, seq_len, total_instances, batch_size, lr, epochs):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    actor = Actor(embedding_dim, hidden_dim)\n",
    "    critic = Critic(hidden_dim)\n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    actor.to(device)\n",
    "    critic.to(device)\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches_per_total_instances = (total_instances + batch_size - 1) // batch_size # Ceiling division\n",
    "\n",
    "    print(f\"Total instances: {total_instances}, Batch size: {batch_size}\")\n",
    "    print(f\"This implies {num_batches_per_total_instances} batches, each trained for {epochs} epochs.\")\n",
    "    print(f\"Starting training...\")\n",
    "\n",
    "    # Outer loop: Iterate through the total number of distinct batches\n",
    "    # Each iteration generates a NEW batch and trains it for 'epochs' times\n",
    "    for batch_iter_idx in tqdm(range(num_batches_per_total_instances), desc=\"Processing Batches\"):\n",
    "        # Generate a NEW batch of TSP problems for this iteration\n",
    "        input_coords = torch.rand(batch_size, seq_len, 2, device=device) # Generates a fresh batch\n",
    "\n",
    "        # Inner loop: Train this specific batch for 'epochs' times\n",
    "        for epoch_idx in range(epochs):\n",
    "            # Actor forward pass generates tours and log probabilities for the current batch\n",
    "            log_probs_batch, tours_batch, encoder_outputs = actor(input_coords, seq_len)\n",
    "\n",
    "            # Calculate rewards for the current batch of tours\n",
    "            # input_coords: (B, S, 2)\n",
    "            # tours_batch: (B, seq_len) - indices\n",
    "            \n",
    "            # gathered_coords needs to gather along dim 1 of input_coords (which has shape 2)\n",
    "            # tours_batch needs to be expanded to (B, seq_len, 2) to gather both x and y coords\n",
    "            gather_indices_coords = tours_batch.unsqueeze(-1).expand(-1, -1, 2)\n",
    "            gathered_coords = torch.gather(input_coords, 1, gather_indices_coords)\n",
    "            \n",
    "            # Append starting node to complete the cycle for each tour in the batch\n",
    "            first_nodes = gathered_coords[:, 0, :].unsqueeze(1) \n",
    "            paths = torch.cat([gathered_coords, first_nodes], dim=1)\n",
    "            \n",
    "            # Calculate distances for each segment in each tour\n",
    "            segment_distances = torch.norm(paths[:, 1:, :] - paths[:, :-1, :], dim=2)\n",
    "            \n",
    "            # Sum distances for each tour to get total reward (negative of total distance)\n",
    "            rewards = -segment_distances.sum(dim=1)\n",
    "\n",
    "            # Critic predicts value for the batch\n",
    "            value = critic(encoder_outputs)\n",
    "            \n",
    "            # Calculate advantage for each item in the batch\n",
    "            advantage = rewards.detach() - value\n",
    "            \n",
    "            # Actor Loss: Sum of log_probs (for each tour) multiplied by advantage (for each tour), then mean over batch\n",
    "            actor_loss = (-log_probs_batch.sum(dim=0) * advantage.detach()).mean()\n",
    "\n",
    "            # Critic Loss: MSE between predicted value and actual reward (detached)\n",
    "            critic_loss = (value - rewards.detach()).pow(2).mean()\n",
    "\n",
    "            # Optimization steps\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True) \n",
    "            critic_optim.step()\n",
    "            \n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "        # Logging after each 'num_batches_per_total_instances' iteration (which includes 'epochs' steps)\n",
    "        # You might want more granular logging, but for now, let's log the last epoch's results for this batch\n",
    "        if (batch_iter_idx + 1) % 100 == 0 or batch_iter_idx == num_batches_per_total_instances - 1: # Log every 10 batch iterations or at the end\n",
    "            print(f\"Batch Iteration {batch_iter_idx+1}/{num_batches_per_total_instances} (Epochs done for this batch): \"\n",
    "                  f\"Actor Loss: {actor_loss.item():.4f}, \"\n",
    "                  f\"Critic Loss: {critic_loss.item():.4f}, \"\n",
    "                  f\"Mean Reward: {rewards.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save({\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optim.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optim.state_dict()\n",
    "    }, 'tsp_actor_critic_nested_batch_epochs.pth')\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    embedding_dim = 128\n",
    "    hidden_dim = 128\n",
    "    seq_len = 20  # Number of cities in TSP\n",
    "    total_training_instances = 100000 # Your total number of instances\n",
    "    batch_size = 256 # Size of each 'mini-batch' that gets trained for 'epochs'\n",
    "    lr = 1e-4\n",
    "    epochs_per_batch = 20 # Number of times each generated batch is trained\n",
    "\n",
    "    print(f\"Starting training with total_instances={total_training_instances}, \"\n",
    "          f\"batch_size={batch_size}, \"\n",
    "          f\"epochs_per_batch={epochs_per_batch}.\")\n",
    "    \n",
    "    train_drl_ac(embedding_dim, hidden_dim, seq_len, \n",
    "                 total_training_instances, batch_size, lr, epochs_per_batch)\n",
    "    \n",
    "    print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329a35b",
   "metadata": {},
   "source": [
    "#### Inference 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # For easier coordinate handling with matplotlib\n",
    "\n",
    "# ----- Actor (Unified Class) - REPEATED FOR COMPLETENESS -----\n",
    "# Make sure this class definition is IDENTICAL to the one used for training\n",
    "# so that the loaded state_dict matches the model's structure.\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Encoder part\n",
    "        self.ff_layer = nn.Linear(2, embedding_dim)\n",
    "        self.gru_encoder = nn.GRU(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
    "\n",
    "        # Decoder part (which includes Attention)\n",
    "        self.gru_decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = self._Attention(hidden_dim) # Using inner Attention class\n",
    "\n",
    "    class _Attention(nn.Module):\n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        def forward(self, decoder_hidden, encoder_outputs, mask):\n",
    "            decoder_hidden_expanded = decoder_hidden.transpose(0, 1) \n",
    "            score = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_expanded)))\n",
    "            score = score.squeeze(-1) \n",
    "            score[mask == 0] = -1e9 # Mask visited\n",
    "            attn_weights = F.softmax(score, dim=-1)\n",
    "            return attn_weights\n",
    "\n",
    "    def forward(self, input_coords, seq_len):\n",
    "        # input_coords: (B, S, 2)\n",
    "\n",
    "        embedded = self.ff_layer(input_coords)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(embedded)\n",
    "\n",
    "        batch_size = input_coords.size(0)\n",
    "        \n",
    "        mask = torch.ones(batch_size, seq_len, device=input_coords.device)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding_dim, device=input_coords.device)\n",
    "        hidden = encoder_hidden \n",
    "        \n",
    "        tours_batch = [] \n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            output, hidden = self.gru_decoder(decoder_input, hidden)\n",
    "            attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "            \n",
    "            selected = torch.argmax(attn_weights, dim=-1) # (B,)\n",
    "            \n",
    "            tours_batch.append(selected)\n",
    "            \n",
    "            mask.scatter_(1, selected.unsqueeze(-1), 0)\n",
    "            \n",
    "            gather_indices = selected.unsqueeze(1).unsqueeze(2).expand(-1, 1, self.embedding_dim)\n",
    "            decoder_input = torch.gather(encoder_outputs, 1, gather_indices) # No .detach() needed in no_grad block\n",
    "\n",
    "        tours_batch_tensor = torch.stack(tours_batch, dim=1) # (B, seq_len)\n",
    "        \n",
    "        return tours_batch_tensor, encoder_outputs \n",
    "\n",
    "\n",
    "# ----- Functions for Loading and Inference -----\n",
    "\n",
    "def load_actor_model(model_path, embedding_dim=128, hidden_dim=128, device='cpu'):\n",
    "    \"\"\"\n",
    "    Loads a trained Actor model from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model checkpoint (.pth file).\n",
    "        embedding_dim (int): Embedding dimension used during training.\n",
    "        hidden_dim (int): Hidden dimension used during training.\n",
    "        device (str or torch.device): Device to load the model onto ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        Actor: The loaded Actor model in evaluation mode.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    actor = Actor(embedding_dim, hidden_dim).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    \n",
    "    actor.eval() # Set the model to evaluation mode\n",
    "    print(f\"Actor model loaded from {model_path} and set to evaluation mode on {device}.\")\n",
    "    return actor\n",
    "\n",
    "def infer_tour_single_instance(actor_model, input_coords):\n",
    "    \"\"\"\n",
    "    Infers the optimal tour for a single given set of input coordinates\n",
    "    using the loaded Actor model.\n",
    "\n",
    "    Args:\n",
    "        actor_model (Actor): The loaded Actor model in evaluation mode.\n",
    "        input_coords (torch.Tensor): A tensor of input coordinates for a SINGLE instance.\n",
    "                                      Shape: (Seq_len, 2)\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - inferred_tour_indices (list): The sequence of visited node indices (Python list).\n",
    "            - inferred_tour_coords (np.ndarray): The corresponding coordinates of the inferred tour.\n",
    "                                                  Shape: (Seq_len, 2)\n",
    "            - tour_reward (float): The calculated reward (negative tour length) for the inferred tour.\n",
    "    \"\"\"\n",
    "    device = next(actor_model.parameters()).device # Get model's device\n",
    "    \n",
    "    # Add a batch dimension for the Actor's forward pass (expects B, S, 2)\n",
    "    input_coords_batched = input_coords.unsqueeze(0).to(device) # (1, Seq_len, 2)\n",
    "    seq_len = input_coords.shape[0]\n",
    "\n",
    "    with torch.no_grad(): # No gradient calculation needed for inference\n",
    "        # The forward method returns tours_batch_tensor and encoder_outputs\n",
    "        predicted_tours_batched, _ = actor_model(input_coords_batched, seq_len)\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        predicted_tours = predicted_tours_batched.squeeze(0) # (Seq_len,)\n",
    "\n",
    "        # Calculate reward for the inferred tour\n",
    "        # tours_batch: (Seq_len,) - indices\n",
    "        \n",
    "        # Gather coordinates for the single tour\n",
    "        # input_coords: (Seq_len, 2)\n",
    "        # predicted_tours.unsqueeze(-1) expands (Seq_len,) to (Seq_len, 1)\n",
    "        # .expand(-1, -1, 2) expands to (Seq_len, 1, 2) -- this is for gather.\n",
    "        # However, for single instance, simpler numpy indexing might be more direct.\n",
    "        # Let's use torch.gather consistent with the batch implementation for clarity\n",
    "        # but apply it after unsqueezing input_coords for the gather op.\n",
    "        \n",
    "        # Original input_coords is (Seq_len, 2)\n",
    "        # We need to gather (Seq_len, 2) from (Seq_len, 2) using (Seq_len, 1) indices\n",
    "        # So expand indices to (Seq_len, 2) for gather.\n",
    "        gather_indices_coords = predicted_tours.unsqueeze(-1).expand(-1, 2) # (Seq_len, 2)\n",
    "        \n",
    "        # input_coords needs to be unsqueezed at dim 0 for gather to work as desired here\n",
    "        # torch.gather(input_coords_batched, 1, predicted_tours_batched.unsqueeze(-1).expand(-1, -1, 2))\n",
    "        # No, input_coords in this function is (seq_len, 2) not (B, S, 2)\n",
    "        # Let's adjust gathering to work on the single instance\n",
    "        \n",
    "        # Simplest way: use numpy indexing after converting to numpy\n",
    "        inferred_tour_indices = predicted_tours.cpu().numpy().tolist() # Convert to list\n",
    "        \n",
    "        # Get coordinates in the inferred order\n",
    "        original_coords_np = input_coords.cpu().numpy() # (Seq_len, 2)\n",
    "        inferred_tour_coords_np = original_coords_np[inferred_tour_indices] # (Seq_len, 2)\n",
    "        \n",
    "        # Calculate reward (tour length)\n",
    "        # Complete the cycle by adding the first point again\n",
    "        path_coords = np.vstack([inferred_tour_coords_np, inferred_tour_coords_np[0]])\n",
    "        \n",
    "        # Calculate Euclidean distances between consecutive points\n",
    "        distances = np.linalg.norm(path_coords[1:] - path_coords[:-1], axis=1)\n",
    "        tour_length = distances.sum()\n",
    "        # tour_reward = -tour_length # Reward is negative tour length\n",
    "\n",
    "    return inferred_tour_indices, inferred_tour_coords_np, tour_length\n",
    "\n",
    "def plot_tour(original_coords, inferred_tour_coords, tour_length):\n",
    "    \"\"\"\n",
    "    Plots the inferred TSP tour, marking the visiting order, start, and end points.\n",
    "\n",
    "    Args:\n",
    "        original_coords (torch.Tensor or np.ndarray): The original coordinates of all cities. (Seq_len, 2)\n",
    "        inferred_tour_coords (np.ndarray): The coordinates of the cities in the inferred tour order. (Seq_len, 2)\n",
    "        tour_length (float): The  tour length.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot all original cities as faint points\n",
    "    ax.scatter(original_coords[:, 0], original_coords[:, 1], color='lightgray', marker='o', s=50, zorder=1, label='All Taksk Points')\n",
    "\n",
    "    # Plot the inferred tour path\n",
    "    # Add the starting point at the end to close the cycle for plotting\n",
    "    plot_path_coords = np.vstack([inferred_tour_coords, inferred_tour_coords[0]])\n",
    "    ax.plot(plot_path_coords[:, 0], plot_path_coords[:, 1], 'b-o', linewidth=1.5, markersize=8, label='Inferred Tour Path', zorder=2)\n",
    "\n",
    "    # Mark the visiting order and points\n",
    "    for i, (x, y) in enumerate(inferred_tour_coords):\n",
    "        ax.text(x + 0.01, y + 0.01, str(i + 1), fontsize=9, ha='center', va='center', color='black', weight='bold', zorder=3)\n",
    "        ax.scatter(x, y, color='blue', s=100, zorder=3) # Highlight visited cities\n",
    "\n",
    "    # Mark start point (first city in the tour)\n",
    "    start_x, start_y = inferred_tour_coords[0]\n",
    "    ax.plot(start_x, start_y, 'go', markersize=12, label='Start Point', zorder=4)\n",
    "    ax.text(start_x + 0.02, start_y + 0.02, 'S', fontsize=12, color='darkgreen', weight='bold', ha='center', va='center', zorder=5)\n",
    "\n",
    "    # Mark end point (which is the same as start point for a closed tour)\n",
    "    # The last city before returning to start point\n",
    "    end_x, end_y = inferred_tour_coords[-1]\n",
    "    ax.plot(end_x, end_y, 'ro', markersize=12, label='End Point', zorder=4)\n",
    "    ax.text(end_x - 0.02, end_y - 0.02, 'E', fontsize=12, color='darkred', weight='bold', ha='center', va='center', zorder=5)\n",
    "\n",
    "\n",
    "    ax.set_title(f'Inferred Robot Tour (Length: {tour_length:.4f})')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d210a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # Define the same hyperparameters used during training\n",
    "    embedding_dim = 128\n",
    "    hidden_dim = 128\n",
    "    seq_len = 40 # Number of tasks\n",
    "\n",
    "    # Define the path to your saved model\n",
    "    # Make sure this matches the path used in train_drl_ac\n",
    "    model_path = 'tsp_actor_critic_nested_batch_epochs.pth' \n",
    "\n",
    "    # Determine device for inference\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Inference will run on: {device}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the Actor model\n",
    "        loaded_actor = load_actor_model(model_path, embedding_dim, hidden_dim, device)\n",
    "\n",
    "        # 2. Prepare a single new input instance for inference\n",
    "        print(\"\\n--- Inferring for a single TSP instance ---\")\n",
    "         \n",
    "        single_instance_coords = torch.rand(seq_len, 2) # (Seq_len, 2) for one problem\n",
    "        \n",
    "        # Convert to float32 if your model was trained with float32 (standard)\n",
    "        single_instance_coords = single_instance_coords.float()\n",
    "\n",
    "        # 3. Perform inference\n",
    "        inferred_tour_indices, inferred_coords_np, tour_length = infer_tour_single_instance(loaded_actor, single_instance_coords)\n",
    "        \n",
    "        # print(f\"Input coordinates (first 3): \\n{single_instance_coords[:3].numpy()}\")\n",
    "        print(f\"Inferred tour (indices): {inferred_tour_indices}\")\n",
    "        print(f\"Reward for single instance: {tour_length:.4f}\")\n",
    "\n",
    "        # 4. Plot the tour\n",
    "        plot_tour(single_instance_coords.numpy(), inferred_coords_np, tour_length)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}.\")\n",
    "        print(\"Please ensure you have run the training script and the model file exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during inference: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc6e3a",
   "metadata": {},
   "source": [
    "#### New Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- Actor (Unified Class) -----\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Encoder part\n",
    "        self.ff_layer = nn.Linear(2, embedding_dim)\n",
    "        self.gru_encoder = nn.GRU(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
    "\n",
    "        # Decoder part (which includes Attention)\n",
    "        self.gru_decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = self._Attention(hidden_dim) # Using inner Attention class\n",
    "\n",
    "    # Inner Attention class (private to Actor, or could be a separate helper)\n",
    "    class _Attention(nn.Module):\n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        def forward(self, decoder_hidden, encoder_outputs, mask):\n",
    "            # decoder_hidden: (1, B, H)\n",
    "            # encoder_outputs: (B, S, H)\n",
    "            \n",
    "            # (1, B, H) -> (B, 1, H) for broadcasting with encoder_outputs\n",
    "            decoder_hidden_expanded = decoder_hidden.transpose(0, 1) \n",
    "            \n",
    "            # Calculate score: (B, S, H) + (B, 1, H) -> (B, S, H) -> (B, S, 1)\n",
    "            score = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_expanded)))\n",
    "            score = score.squeeze(-1) # (B, S)\n",
    "\n",
    "            # Mask visited (B, S)\n",
    "            score[mask == 0] = -1e9 \n",
    "            \n",
    "            attn_weights = F.softmax(score, dim=-1) # (B, S)\n",
    "            return attn_weights\n",
    "\n",
    "    def forward(self, input_coords, seq_len):\n",
    "        # input_coords: (B, S, 2)\n",
    "\n",
    "        # Encoder Pass\n",
    "        # embedded: (B, S, E)\n",
    "        # encoder_outputs: (B, S, H)\n",
    "        # encoder_hidden: (1, B, H)\n",
    "        embedded = self.ff_layer(input_coords)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(embedded)\n",
    "\n",
    "        batch_size = input_coords.size(0)\n",
    "        \n",
    "        # Initial decoder input: Embedding of the robot station (which is the first city, index 0)\n",
    "        # 'embedded' is (B, S, E), so embedded[:, 0, :] is (B, E). Unsqueeze(1) makes it (B, 1, E).\n",
    "        decoder_input = embedded[:, 0, :].unsqueeze(1) # (B, 1, E)\n",
    "        \n",
    "        hidden = encoder_hidden # Initial decoder hidden state from encoder\n",
    "        \n",
    "        # Initialize mask: Mark the first city (robot station, index 0) as visited\n",
    "        mask = torch.ones(batch_size, seq_len, device=input_coords.device)\n",
    "        mask[:, 0] = 0 # Robot station (index 0) is already visited\n",
    "\n",
    "        log_probs_batch = [] \n",
    "        # The tour starts with the robot station (index 0) for all batches\n",
    "        tours_batch = [torch.zeros(batch_size, dtype=torch.long, device=input_coords.device)] \n",
    "\n",
    "        # The loop runs for seq_len - 1 steps to select the remaining cities\n",
    "        for _ in range(seq_len - 1): # We need to select seq_len - 1 other cities\n",
    "            output, hidden = self.gru_decoder(decoder_input, hidden)\n",
    "            attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "            \n",
    "            dist = torch.distributions.Categorical(attn_weights)\n",
    "            selected = dist.sample() # (B,)\n",
    "            \n",
    "            log_prob = dist.log_prob(selected) # (B,)\n",
    "            log_probs_batch.append(log_prob)\n",
    "            \n",
    "            tours_batch.append(selected)\n",
    "            \n",
    "            # Update mask: Set selected nodes to 0 for each item in the batch\n",
    "            # selected.unsqueeze(-1) changes (B,) to (B, 1) for scatter_\n",
    "            mask.scatter_(1, selected.unsqueeze(-1), 0)\n",
    "            \n",
    "            # --- CORRECT FIX for torch.gather index shape ---\n",
    "            # selected: (B,)\n",
    "            # unsqueeze(1) -> (B, 1)\n",
    "            # unsqueeze(2) -> (B, 1, 1)\n",
    "            # expand(-1, 1, self.embedding_dim) -> (B, 1, embedding_dim)\n",
    "            # This 'gather_indices' tensor now has the same number of dimensions as encoder_outputs\n",
    "            # and is correctly shaped to select an embedding vector for each batch item.\n",
    "            gather_indices = selected.unsqueeze(1).unsqueeze(2).expand(-1, 1, self.embedding_dim)\n",
    "            decoder_input = torch.gather(encoder_outputs, 1, gather_indices).detach()\n",
    "            # decoder_input will be (B, 1, E) as required for the next GRU step\n",
    "            # --- END CORRECT FIX ---\n",
    "\n",
    "        tours_batch_tensor = torch.stack(tours_batch, dim=1) # (B, seq_len)\n",
    "        log_probs_batch_tensor = torch.stack(log_probs_batch, dim=0) # (seq_len, B)\n",
    "\n",
    "        return log_probs_batch_tensor, tours_batch_tensor, encoder_outputs \n",
    "\n",
    "\n",
    "# ----- Critic -----\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        pooled = encoder_outputs.mean(dim=1) # (B, H)\n",
    "        return self.fc(pooled).squeeze(-1) # (B,)\n",
    "\n",
    "# ----- Training -----\n",
    "\n",
    "def train_drl_ac(embedding_dim, hidden_dim, seq_len, total_instances, batch_size, lr, epochs):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    actor = Actor(embedding_dim, hidden_dim)\n",
    "    critic = Critic(hidden_dim)\n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    actor.to(device)\n",
    "    critic.to(device)\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches_per_total_instances = (total_instances + batch_size - 1) // batch_size # Ceiling division\n",
    "\n",
    "    print(f\"Total instances: {total_instances}, Batch size: {batch_size}\")\n",
    "    print(f\"This implies {num_batches_per_total_instances} batches, each trained for {epochs} epochs.\")\n",
    "    print(f\"Starting training...\")\n",
    "\n",
    "    # Outer loop: Iterate through the total number of distinct batches\n",
    "    # Each iteration generates a NEW batch and trains it for 'epochs' times\n",
    "    for batch_iter_idx in tqdm(range(num_batches_per_total_instances), desc=\"Processing Batches\"):\n",
    "        # Define the fixed robot station coordinates (e.g., at [0.5, 0.5])\n",
    "        # Ensure it's on the correct device and expanded to match the batch_size\n",
    "        robot_station_coords_fixed = torch.tensor([[0.55, 0.85]], dtype=torch.float32, device=device).expand(batch_size, 1, 2)\n",
    "\n",
    "        # Generate random coordinates for the remaining (seq_len - 1) cities\n",
    "        random_other_coords = torch.rand(batch_size, seq_len - 1, 2, device=device)\n",
    "\n",
    "        # Concatenate the robot station as the first city for all instances in the batch\n",
    "        input_coords = torch.cat([robot_station_coords_fixed, random_other_coords], dim=1)\n",
    "\n",
    "\n",
    "        # Inner loop: Train this specific batch for 'epochs' times\n",
    "        for epoch_idx in range(epochs):\n",
    "            # Actor forward pass generates tours and log probabilities for the current batch\n",
    "            log_probs_batch, tours_batch, encoder_outputs = actor(input_coords, seq_len)\n",
    "\n",
    "            # Calculate rewards for the current batch of tours\n",
    "            # input_coords: (B, S, 2)\n",
    "            # tours_batch: (B, seq_len) - indices\n",
    "            \n",
    "            # gathered_coords needs to gather along dim 1 of input_coords (which has shape 2)\n",
    "            # tours_batch needs to be expanded to (B, seq_len, 2) to gather both x and y coords\n",
    "            gather_indices_coords = tours_batch.unsqueeze(-1).expand(-1, -1, 2)\n",
    "            gathered_coords = torch.gather(input_coords, 1, gather_indices_coords)\n",
    "            \n",
    "            # Append starting node to complete the cycle for each tour in the batch\n",
    "            first_nodes = gathered_coords[:, 0, :].unsqueeze(1) \n",
    "            paths = torch.cat([gathered_coords, first_nodes], dim=1)\n",
    "            \n",
    "            # Calculate distances for each segment in each tour\n",
    "            segment_distances = torch.norm(paths[:, 1:, :] - paths[:, :-1, :], dim=2)\n",
    "            \n",
    "            # Sum distances for each tour to get total reward (negative of total distance)\n",
    "            rewards = -segment_distances.sum(dim=1)\n",
    "\n",
    "            # Critic predicts value for the batch\n",
    "            value = critic(encoder_outputs)\n",
    "            \n",
    "            # Calculate advantage for each item in the batch\n",
    "            advantage = rewards.detach() - value\n",
    "            \n",
    "            # Actor Loss: Sum of log_probs (for each tour) multiplied by advantage (for each tour), then mean over batch\n",
    "            actor_loss = (-log_probs_batch.sum(dim=0) * advantage.detach()).mean()\n",
    "\n",
    "            # Critic Loss: MSE between predicted value and actual reward (detached)\n",
    "            critic_loss = (value - rewards.detach()).pow(2).mean()\n",
    "\n",
    "            # Optimization steps\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True) \n",
    "            critic_optim.step()\n",
    "            \n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "        # Logging after each 'num_batches_per_total_instances' iteration (which includes 'epochs' steps)\n",
    "        # You might want more granular logging, but for now, let's log the last epoch's results for this batch\n",
    "        if (batch_iter_idx + 1) % 100 == 0 or batch_iter_idx == num_batches_per_total_instances - 1: # Log every 10 batch iterations or at the end\n",
    "            print(f\"Batch Iteration {batch_iter_idx+1}/{num_batches_per_total_instances} (Epochs done for this batch): \"\n",
    "                  f\"Actor Loss: {actor_loss.item():.4f}, \"\n",
    "                  f\"Critic Loss: {critic_loss.item():.4f}, \"\n",
    "                  f\"Mean Reward: {rewards.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save({\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optim.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optim.state_dict()\n",
    "    }, 'tsp_ac_256_1L.pth')\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 256\n",
    "    seq_len = 50  # Number of cities in TSP\n",
    "    total_training_instances = 100000 # Your total number of instances\n",
    "    batch_size = 256 # Size of each 'mini-batch' that gets trained for 'epochs'\n",
    "    lr = 5e-4\n",
    "    epochs_per_batch = 20 # Number of times each generated batch is trained\n",
    "\n",
    "    print(f\"Starting training with total_instances={total_training_instances}, \"\n",
    "          f\"batch_size={batch_size}, \"\n",
    "          f\"epochs_per_batch={epochs_per_batch}.\")\n",
    "    \n",
    "    train_drl_ac(embedding_dim, hidden_dim, seq_len, \n",
    "                 total_training_instances, batch_size, lr, epochs_per_batch)\n",
    "    \n",
    "    print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e957615",
   "metadata": {},
   "source": [
    "#### Infer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f0b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # For easier coordinate handling with matplotlib\n",
    "\n",
    "# ----- Actor (Unified Class) - REPEATED FOR COMPLETENESS -----\n",
    "# Make sure this class definition is IDENTICAL to the one used for training\n",
    "# so that the loaded state_dict matches the model's structure.\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Encoder part\n",
    "        self.ff_layer = nn.Linear(2, embedding_dim)\n",
    "        self.gru_encoder = nn.GRU(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
    "\n",
    "        # Decoder part (which includes Attention)\n",
    "        self.gru_decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = self._Attention(hidden_dim) # Using inner Attention class\n",
    "\n",
    "    class _Attention(nn.Module):\n",
    "        def __init__(self, hidden_dim):\n",
    "            super().__init__()\n",
    "            self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        def forward(self, decoder_hidden, encoder_outputs, mask):\n",
    "            decoder_hidden_expanded = decoder_hidden.transpose(0, 1) \n",
    "            score = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_expanded)))\n",
    "            score = score.squeeze(-1) \n",
    "            score[mask == 0] = -1e9 # Mask visited\n",
    "            attn_weights = F.softmax(score, dim=-1)\n",
    "            return attn_weights\n",
    "\n",
    "    def forward(self, input_coords, seq_len):\n",
    "        # input_coords: (B, S, 2)\n",
    "\n",
    "        embedded = self.ff_layer(input_coords)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(embedded)\n",
    "\n",
    "        batch_size = input_coords.size(0)\n",
    "        \n",
    "        mask = torch.ones(batch_size, seq_len, device=input_coords.device)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding_dim, device=input_coords.device)\n",
    "        hidden = encoder_hidden \n",
    "        \n",
    "        tours_batch = [] \n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            output, hidden = self.gru_decoder(decoder_input, hidden)\n",
    "            attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "            \n",
    "            selected = torch.argmax(attn_weights, dim=-1) # (B,)\n",
    "            \n",
    "            tours_batch.append(selected)\n",
    "            \n",
    "            mask.scatter_(1, selected.unsqueeze(-1), 0)\n",
    "            \n",
    "            gather_indices = selected.unsqueeze(1).unsqueeze(2).expand(-1, 1, self.embedding_dim)\n",
    "            decoder_input = torch.gather(encoder_outputs, 1, gather_indices) # No .detach() needed in no_grad block\n",
    "\n",
    "        tours_batch_tensor = torch.stack(tours_batch, dim=1) # (B, seq_len)\n",
    "        \n",
    "        return tours_batch_tensor, encoder_outputs \n",
    "\n",
    "# ----- Functions for Loading and Inference -----\n",
    "\n",
    "def load_actor_model(model_path, embedding_dim=128, hidden_dim=128, device='cpu'):\n",
    "    \"\"\"\n",
    "    Loads a trained Actor model from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model checkpoint (.pth file).\n",
    "        embedding_dim (int): Embedding dimension used during training.\n",
    "        hidden_dim (int): Hidden dimension used during training.\n",
    "        device (str or torch.device): Device to load the model onto ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        Actor: The loaded Actor model in evaluation mode.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    actor = Actor(embedding_dim, hidden_dim).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    \n",
    "    actor.eval() # Set the model to evaluation mode\n",
    "    print(f\"Actor model loaded from {model_path} and set to evaluation mode on {device}.\")\n",
    "    return actor\n",
    "\n",
    "def infer_tour_single_instance(actor_model, input_coords):\n",
    "    \"\"\"\n",
    "    Infers the optimal tour for a single given set of input coordinates\n",
    "    using the loaded Actor model.\n",
    "\n",
    "    Args:\n",
    "        actor_model (Actor): The loaded Actor model in evaluation mode.\n",
    "        input_coords (torch.Tensor): A tensor of input coordinates for a SINGLE instance.\n",
    "                                      Shape: (Seq_len, 2)\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - inferred_tour_indices (list): The sequence of visited node indices (Python list).\n",
    "            - inferred_tour_coords (np.ndarray): The corresponding coordinates of the inferred tour.\n",
    "                                                  Shape: (Seq_len, 2)\n",
    "            - tour_reward (float): The calculated reward (negative tour length) for the inferred tour.\n",
    "    \"\"\"\n",
    "    device = next(actor_model.parameters()).device # Get model's device\n",
    "    \n",
    "    # Add a batch dimension for the Actor's forward pass (expects B, S, 2)\n",
    "    input_coords_batched = input_coords.unsqueeze(0).to(device) # (1, Seq_len, 2)\n",
    "    seq_len = input_coords.shape[0]\n",
    "\n",
    "    with torch.no_grad(): # No gradient calculation needed for inference\n",
    "        # The forward method returns tours_batch_tensor and encoder_outputs\n",
    "        predicted_tours_batched, _ = actor_model(input_coords_batched, seq_len)\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        predicted_tours = predicted_tours_batched.squeeze(0) # (Seq_len,)\n",
    "\n",
    "        # Calculate reward for the inferred tour\n",
    "        # tours_batch: (Seq_len,) - indices\n",
    "        \n",
    "        # Gather coordinates for the single tour\n",
    "        # input_coords: (Seq_len, 2)\n",
    "        # predicted_tours.unsqueeze(-1) expands (Seq_len,) to (Seq_len, 1)\n",
    "        # .expand(-1, -1, 2) expands to (Seq_len, 1, 2) -- this is for gather.\n",
    "        # However, for single instance, simpler numpy indexing might be more direct.\n",
    "        # Let's use torch.gather consistent with the batch implementation for clarity\n",
    "        # but apply it after unsqueezing input_coords for the gather op.\n",
    "        \n",
    "        # Original input_coords is (Seq_len, 2)\n",
    "        # We need to gather (Seq_len, 2) from (Seq_len, 2) using (Seq_len, 1) indices\n",
    "        # So expand indices to (Seq_len, 2) for gather.\n",
    "        gather_indices_coords = predicted_tours.unsqueeze(-1).expand(-1, 2) # (Seq_len, 2)\n",
    "        \n",
    "        # input_coords needs to be unsqueezed at dim 0 for gather to work as desired here\n",
    "        # torch.gather(input_coords_batched, 1, predicted_tours_batched.unsqueeze(-1).expand(-1, -1, 2))\n",
    "        # No, input_coords in this function is (seq_len, 2) not (B, S, 2)\n",
    "        # Let's adjust gathering to work on the single instance\n",
    "        \n",
    "        # Simplest way: use numpy indexing after converting to numpy\n",
    "        inferred_tour_indices = predicted_tours.cpu().numpy().tolist() # Convert to list\n",
    "        \n",
    "        # Get coordinates in the inferred order\n",
    "        original_coords_np = input_coords.cpu().numpy() # (Seq_len, 2)\n",
    "        inferred_tour_coords_np = original_coords_np[inferred_tour_indices] # (Seq_len, 2)\n",
    "        \n",
    "        # Calculate reward (tour length)\n",
    "        # Complete the cycle by adding the first point again\n",
    "        path_coords = np.vstack([inferred_tour_coords_np, inferred_tour_coords_np[0]])\n",
    "        \n",
    "        # Calculate Euclidean distances between consecutive points\n",
    "        distances = np.linalg.norm(path_coords[1:] - path_coords[:-1], axis=1)\n",
    "        tour_length = distances.sum()\n",
    "        # tour_reward = -tour_length # Reward is negative tour length\n",
    "\n",
    "    return inferred_tour_indices, inferred_tour_coords_np, tour_length\n",
    "\n",
    "def infer_tour_batch(actor_model, batch_coords):\n",
    "    \"\"\"\n",
    "    Infers tours for a batch of input coordinate sets using the loaded Actor model.\n",
    "\n",
    "    Args:\n",
    "        actor_model (Actor): The loaded Actor model in evaluation mode.\n",
    "        batch_coords (torch.Tensor): Tensor of input coordinates for a batch.\n",
    "                                        Shape: (batch_size, seq_len, 2)\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - inferred_tour_indices_list (list of lists): Each sublist is the sequence of visited node indices.\n",
    "            - inferred_tour_coords_list (list of np.ndarray): Each array is (seq_len, 2) of inferred tour coordinates.\n",
    "            - tour_lengths (list of float): Tour lengths for each instance in the batch.\n",
    "    \"\"\"\n",
    "    device = next(actor_model.parameters()).device\n",
    "    batch_coords = batch_coords.to(device)\n",
    "    batch_size, seq_len, _ = batch_coords.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_tours_batched, _ = actor_model(batch_coords, seq_len)  # (B, seq_len)\n",
    "        inferred_tour_indices_list = []\n",
    "        inferred_tour_coords_list = []\n",
    "        tour_lengths = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            predicted_tour = predicted_tours_batched[i].cpu().numpy().tolist()\n",
    "            coords_np = batch_coords[i].cpu().numpy()\n",
    "            inferred_coords = coords_np[predicted_tour]\n",
    "            path_coords = np.vstack([inferred_coords, inferred_coords[0]])\n",
    "            distances = np.linalg.norm(path_coords[1:] - path_coords[:-1], axis=1)\n",
    "            tour_length = distances.sum()\n",
    "\n",
    "            inferred_tour_indices_list.append(predicted_tour)\n",
    "            inferred_tour_coords_list.append(inferred_coords)\n",
    "            tour_lengths.append(tour_length)\n",
    "\n",
    "    return inferred_tour_indices_list, inferred_tour_coords_list, tour_lengths\n",
    "\n",
    "def plot_tour(original_coords, inferred_tour_coords, tour_length):\n",
    "    \"\"\"\n",
    "    Plots the inferred TSP tour, marking the visiting order, start, and end points.\n",
    "\n",
    "    Args:\n",
    "        original_coords (torch.Tensor or np.ndarray): The original coordinates of all cities. (Seq_len, 2)\n",
    "        inferred_tour_coords (np.ndarray): The coordinates of the cities in the inferred tour order. (Seq_len, 2)\n",
    "        tour_length (float): The  tour length.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot all original cities as faint points\n",
    "    ax.scatter(original_coords[:, 0], original_coords[:, 1], color='lightgray', marker='o', s=50, zorder=1, label='All Taksk Points')\n",
    "\n",
    "    # Plot the inferred tour path\n",
    "    # Add the starting point at the end to close the cycle for plotting\n",
    "    plot_path_coords = np.vstack([inferred_tour_coords, inferred_tour_coords[0]])\n",
    "    ax.plot(plot_path_coords[:, 0], plot_path_coords[:, 1], 'b-o', linewidth=1.5, markersize=8, label='Inferred Tour Path', zorder=2)\n",
    "\n",
    "    # Mark the visiting order and points\n",
    "    for i, (x, y) in enumerate(inferred_tour_coords):\n",
    "        ax.text(x + 0.01, y + 0.01, str(i + 1), fontsize=9, ha='center', va='center', color='black', weight='bold', zorder=3)\n",
    "        ax.scatter(x, y, color='blue', s=100, zorder=3) # Highlight visited cities\n",
    "\n",
    "    # Mark start point (first city in the tour)\n",
    "    start_x, start_y = inferred_tour_coords[0]\n",
    "    ax.plot(start_x, start_y, 'go', markersize=12, label='Start Point', zorder=4)\n",
    "    ax.text(start_x + 0.02, start_y + 0.02, 'S', fontsize=12, color='darkgreen', weight='bold', ha='center', va='center', zorder=5)\n",
    "\n",
    "    # Mark end point (which is the same as start point for a closed tour)\n",
    "    # The last city before returning to start point\n",
    "    end_x, end_y = inferred_tour_coords[-1]\n",
    "    ax.plot(end_x, end_y, 'ro', markersize=12, label='End Point', zorder=4)\n",
    "    ax.text(end_x - 0.02, end_y - 0.02, 'E', fontsize=12, color='darkred', weight='bold', ha='center', va='center', zorder=5)\n",
    "\n",
    "\n",
    "    ax.set_title(f'Inferred Robot Tour (Length: {tour_length:.4f})')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # Define the same hyperparameters used during training\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 256\n",
    "    seq_len = 20 # Number of tasks\n",
    "\n",
    "    # Define the path to your saved model\n",
    "    # Make sure this matches the path used in train_drl_ac\n",
    "    model_path = 'tsp_ac_256_1L.pth' \n",
    "\n",
    "    # Determine device for inference\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Inference will run on: {device}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the Actor model\n",
    "        loaded_actor = load_actor_model(model_path, embedding_dim, hidden_dim, device)\n",
    "\n",
    "        # 2. Prepare a single new input instance for inference\n",
    "        print(\"\\n--- Inferring for a single TSP instance ---\")\n",
    "         \n",
    "        single_instance_coords = torch.rand(seq_len, 2) # (Seq_len, 2) for one problem\n",
    "        \n",
    "        # Convert to float32 if your model was trained with float32 (standard)\n",
    "        single_instance_coords = single_instance_coords.float()\n",
    "\n",
    "        # 3. Perform inference\n",
    "        inferred_tour_indices, inferred_coords_np, tour_length = infer_tour_single_instance(loaded_actor, single_instance_coords)\n",
    "        \n",
    "        # print(f\"Input coordinates (first 3): \\n{single_instance_coords[:3].numpy()}\")\n",
    "        print(f\"Inferred tour (indices): {inferred_tour_indices}\")\n",
    "        print(f\"Reward for single instance: {tour_length:.4f}\")\n",
    "\n",
    "        # 4. Plot the tour\n",
    "        plot_tour(single_instance_coords.numpy(), inferred_coords_np, tour_length)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}.\")\n",
    "        print(\"Please ensure you have run the training script and the model file exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference will run on: cuda\n",
      "Actor model loaded from tsp_ac_256_1L.pth and set to evaluation mode on cuda.\n",
      "\n",
      "--- Inferring for a batch of 10 TSP instances ---\n",
      "\n",
      "Instance 1:\n",
      "Inferred tour (indices): [5, 1, 7, 3, 9, 2, 8, 6, 0, 4]\n",
      "Tour length: 2.6496\n",
      "\n",
      "Instance 2:\n",
      "Inferred tour (indices): [0, 3, 5, 2, 9, 6, 1, 8, 7, 4]\n",
      "Tour length: 2.7445\n",
      "\n",
      "Instance 3:\n",
      "Inferred tour (indices): [9, 3, 0, 6, 7, 5, 8, 2, 4, 1]\n",
      "Tour length: 2.9520\n",
      "\n",
      "Instance 4:\n",
      "Inferred tour (indices): [3, 2, 7, 9, 4, 6, 0, 1, 8, 5]\n",
      "Tour length: 3.4137\n",
      "\n",
      "Instance 5:\n",
      "Inferred tour (indices): [2, 0, 6, 4, 8, 9, 7, 5, 3, 1]\n",
      "Tour length: 2.5090\n",
      "\n",
      "Instance 6:\n",
      "Inferred tour (indices): [1, 4, 6, 8, 5, 0, 9, 2, 7, 3]\n",
      "Tour length: 3.6194\n",
      "\n",
      "Instance 7:\n",
      "Inferred tour (indices): [2, 7, 9, 5, 4, 0, 8, 3, 6, 1]\n",
      "Tour length: 3.5971\n",
      "\n",
      "Instance 8:\n",
      "Inferred tour (indices): [2, 0, 9, 4, 8, 6, 7, 5, 3, 1]\n",
      "Tour length: 3.1665\n",
      "\n",
      "Instance 9:\n",
      "Inferred tour (indices): [3, 1, 4, 9, 0, 2, 6, 7, 5, 8]\n",
      "Tour length: 2.6559\n",
      "\n",
      "Instance 10:\n",
      "Inferred tour (indices): [2, 5, 1, 7, 6, 9, 0, 3, 4, 8]\n",
      "Tour length: 3.0840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Temp\\ipykernel_5392\\674207019.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "#--------Batch Inference Functionality--------\n",
    "if __name__ == '__main__':\n",
    "    # Define the same hyperparameters used during training\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 256\n",
    "    seq_len = 10  # Number of tasks\n",
    "    batch_size = 10\n",
    "\n",
    "    model_path = 'tsp_ac_256_1L.pth'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Inference will run on: {device}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the Actor model\n",
    "        loaded_actor = load_actor_model(model_path, embedding_dim, hidden_dim, device)\n",
    "\n",
    "        # 2. Prepare a batch of new input instances for inference\n",
    "        print(f\"\\n--- Inferring for a batch of {batch_size} TSP instances ---\")\n",
    "        batch_coords = torch.rand(batch_size, seq_len, 2).float()\n",
    "\n",
    "        # 3. Perform batch inference\n",
    "        inferred_tour_indices_list, inferred_tour_coords_list, tour_lengths = infer_tour_batch(loaded_actor, batch_coords)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            print(f\"\\nInstance {i+1}:\")\n",
    "            print(f\"Inferred tour (indices): {inferred_tour_indices_list[i]}\")\n",
    "            print(f\"Tour length: {tour_lengths[i]:.4f}\")\n",
    "            # plot_tour(batch_coords[i].numpy(), inferred_tour_coords_list[i], tour_lengths[i])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}.\")\n",
    "        print(\"Please ensure you have run the training script and the model file exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during batch inference: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_mlag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
